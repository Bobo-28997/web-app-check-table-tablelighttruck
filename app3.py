# =====================================
# Streamlit App: äººäº‹ç”¨â€œææˆé¡¹ç›® & äºŒæ¬¡é¡¹ç›® & å¹³å°å·¥ & ç‹¬ç«‹æ¶æ„ & ä½ä»·å€¼ & æƒè´£å‘ç”Ÿâ€è‡ªåŠ¨å®¡æ ¸ï¼ˆç»ˆæä¿®æ­£ç‰ˆï¼‰
# - ä¸¥æ ¼å­—æ®µæ¯”å¯¹
# - æ—¥æœŸå®¹é”™
# - â€œç§ŸèµæœŸé™â€Â±0.5 æœˆï¼ˆç»ç†è¡¨å¹´ -> Ã—12ï¼‰
# - âœ… æ“ä½œäºº vs å®¢æˆ·ç»ç†
# - âœ… äº§å“ vs äº§å“åç§°_å•†
# - âœ… åŸå¸‚ç»ç† vs è¶…æœŸæ˜ç»† åŸå¸‚ç»ç†
# - âœ… æƒè´£å‘ç”Ÿå­—æ®µ vs ç»ç†è¡¨å­—æ®µ
# - âœ… æœ€ç»ˆæ¼å¡«æ£€æµ‹ï¼šä½¿ç”¨â€œæ”¾æ¬¾æ˜ç»†â€ä¸­å«â€œææˆâ€çš„sheetä¸ºåŸºå‡†
# =====================================

import streamlit as st
import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows # <--- æ·»åŠ è¿™ä¸€è¡Œ
from io import BytesIO
import unicodedata, re

st.title("ğŸ“Š æ¨¡æ‹Ÿäººäº‹ç”¨è–ªèµ„è®¡ç®—è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿ-2")

# ========== ä¸Šä¼ æ–‡ä»¶ ==========
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ æ–‡ä»¶åä¸­åŒ…å«ä»¥ä¸‹å­—æ®µçš„æ–‡ä»¶ï¼šææˆé¡¹ç›®ã€æ”¾æ¬¾æ˜ç»†ã€äºŒæ¬¡æ˜ç»†ã€äº§å“å°è´¦ï¼Œè¶…æœŸæ˜ç»†ã€‚æœ€åèªŠå†™ï¼Œéœ€æ£€çš„è¡¨ä¸ºææˆé¡¹ç›®è¡¨ã€‚ä½¿ç”¨2æ•°æ®æ—¶ï¼Œææˆé¡¹ç›®è¡¨ä¸­éœ€æ£€sheetçš„â€˜å¹´é™â€™ç­‰æœˆå•ä½çš„æ—¶é•¿åˆ—åéœ€æ”¹ä¸ºâ€˜ç§ŸèµæœŸé™â€™ã€‚",
    type="xlsx", accept_multiple_files=True
)

if not uploaded_files or len(uploaded_files) < 5:
    st.warning("âš ï¸ è¯·ä¸Šä¼ è‡³å°‘äº”ä¸ªæ–‡ä»¶ï¼ˆææˆé¡¹ç›®ã€äºŒæ¬¡æ˜ç»†ã€æ”¾æ¬¾æ˜ç»†ã€äº§å“å°è´¦ã€è¶…æœŸæ˜ç»†ï¼‰")
    st.stop()
else:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ")

# ========== å·¥å…·å‡½æ•° ==========
def find_file(files_list, keyword):
    for f in files_list:
        if keyword in f.name:
            return f
    raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„æ–‡ä»¶")

def normalize_colname(c):
    return str(c).strip().lower()

def find_col(df, keyword, exact=False):
    if df is None:
        return None
    key = keyword.strip().lower()
    for col in df.columns:
        cname = normalize_colname(col)
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

def normalize_num(val):
    if pd.isna(val):
        return None
    s = str(val).replace(",", "").strip()
    if s in ["", "-", "nan"]:
        return None
    try:
        if "%" in s:
            s = s.replace("%", "")
            return float(s) / 100
        return float(s)
    except:
        return s

def normalize_text(val):
    if pd.isna(val):
        return ""
    s = str(val)
    s = re.sub(r'[\n\r\t ]+', '', s)
    s = s.replace('\u3000', '')
    s = ''.join(unicodedata.normalize('NFKC', ch) for ch in s)
    return s.lower().strip()

def detect_header_row(file, sheet_name):
    preview = pd.read_excel(file, sheet_name=sheet_name, nrows=2, header=None)
    first_row = preview.iloc[0]
    total_cells = len(first_row)
    empty_like = sum(
        (pd.isna(x) or str(x).startswith("Unnamed") or str(x).strip() == "")
        for x in first_row
    )
    empty_ratio = empty_like / total_cells if total_cells > 0 else 0
    return 1 if empty_ratio >= 0.7 else 0

def get_header_row(file, sheet_name):
    if any(k in sheet_name for k in ["èµ·ç§Ÿ", "äºŒæ¬¡"]):
        return 1
    return detect_header_row(file, sheet_name)

def normalize_contract_key(series: pd.Series) -> pd.Series:
    """
    å¯¹åˆåŒå· Series è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œç”¨äºå®‰å…¨çš„ pd.merge æ“ä½œã€‚
    (æ¥è‡ªæˆ‘ä»¬ä¸Šä¸€ä¸ª app çš„ç»éªŒ)
    """
    s = series.astype(str)
    s = s.str.replace(r"\.0$", "", regex=True) 
    s = s.str.strip()
    s = s.str.upper() 
    s = s.str.replace('ï¼', '-', regex=False)
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸ç”¨ normalize_text çš„ r'\s+'
    # å› ä¸ºåˆåŒå·å†…éƒ¨å¯èƒ½å…è®¸æœ‰ç©ºæ ¼
    return s

def prepare_one_ref_df(ref_df, ref_contract_col, required_cols, prefix):
    """
    é¢„å¤„ç†å•ä¸ªå‚è€ƒDataFrameï¼Œæå–æ‰€éœ€åˆ—å¹¶æ ‡å‡†åŒ–Keyã€‚
    (V2: å¢åŠ æœªæ‰¾åˆ°åˆ—çš„è­¦å‘Š)
    """
    if ref_df is None:
        st.warning(f"âš ï¸ å‚è€ƒæ–‡ä»¶ '{prefix}' æœªåŠ è½½ (df is None)ã€‚")
        return pd.DataFrame(columns=['__KEY__'])
        
    if ref_contract_col is None:
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°'åˆåŒ'åˆ—ï¼Œè·³è¿‡æ­¤æ•°æ®æºã€‚")
        return pd.DataFrame(columns=['__KEY__'])

    # æ‰¾å‡ºå®é™…å­˜åœ¨çš„åˆ—
    cols_to_extract = []
    col_mapping = {} # 'åŸå§‹åˆ—å' -> 'ref_prefix_åŸå§‹åˆ—å'

    for col_kw in required_cols:
        actual_col = find_col(ref_df, col_kw)
        
        if actual_col:
            cols_to_extract.append(actual_col)
            # ä½¿ç”¨åŸå§‹åˆ—å (col_kw) ä½œä¸ºæ ‡å‡†åç¼€
            col_mapping[actual_col] = f"ref_{prefix}_{col_kw}"
        else:
            # --- VVVV (ã€æ–°åŠŸèƒ½ã€‘æ·»åŠ è­¦å‘Š) VVVV ---
            st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°åˆ— (å…³é”®å­—: '{col_kw}')")
            # --- ^^^^ (ã€æ–°åŠŸèƒ½ã€‘æ·»åŠ è­¦å‘Š) ^^^^ ---
            
    if not cols_to_extract:
        # å¦‚æœä¸€ä¸ªéœ€è¦çš„åˆ—éƒ½æ²¡æ‰¾åˆ°ï¼Œä¹Ÿä¸ç”¨ç»§ç»­äº†
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°ä»»ä½•æ‰€éœ€å­—æ®µï¼Œè·³è¿‡ã€‚")
        return pd.DataFrame(columns=['__KEY__'])

    # æå–æ‰€éœ€åˆ— + åˆåŒåˆ—
    cols_to_extract.append(ref_contract_col)
    
    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯å”¯ä¸€çš„
    cols_to_extract_unique = list(set(cols_to_extract))
    
    # æ£€æŸ¥æ‰€æœ‰åˆ—æ˜¯å¦çœŸçš„å­˜åœ¨ (set æ“ä½œå¯èƒ½å¼•å…¥ä¸å­˜åœ¨çš„åˆ—, å°½ç®¡è¿™é‡Œä¸å¤ªå¯èƒ½)
    valid_cols = [col for col in cols_to_extract_unique if col in ref_df.columns]
    
    std_df = ref_df[valid_cols].copy()

    # æ ‡å‡†åŒ–Key
    std_df['__KEY__'] = normalize_contract_key(std_df[ref_contract_col])
    
    # é‡å‘½å
    std_df = std_df.rename(columns=col_mapping)
    
    # åªä¿ç•™éœ€è¦çš„åˆ—
    final_cols = ['__KEY__'] + list(col_mapping.values())
    
    # ç¡®ä¿ final_cols éƒ½åœ¨ std_df ä¸­ (å› ä¸ºé‡å‘½åäº†)
    final_cols_in_df = [col for col in final_cols if col in std_df.columns]
    
    std_df = std_df[final_cols_in_df]
    
    # å»é‡
    std_df = std_df.drop_duplicates(subset=['__KEY__'], keep='first')
    return std_df

def compare_series_vec(s_main, s_ref, compare_type='text', tolerance=0, multiplier=1):
    """
    å‘é‡åŒ–æ¯”è¾ƒä¸¤ä¸ªSeriesï¼Œå¤åˆ» compare_and_mark çš„é€»è¾‘ã€‚
    (V2ï¼šå¢åŠ å¯¹ merge å¤±è´¥ (NaN) çš„é™é»˜è·³è¿‡)
    """
    # 0. è¯†åˆ« Merge å¤±è´¥
    merge_failed_mask = s_ref.isna()

    # 1. é¢„å¤„ç†ç©ºå€¼
    main_is_na = pd.isna(s_main) | (s_main.astype(str).str.strip().isin(["", "nan", "None"]))
    ref_is_na = pd.isna(s_ref) | (s_ref.astype(str).str.strip().isin(["", "nan", "None"]))
    both_are_na = main_is_na & ref_is_na
    
    errors = pd.Series(False, index=s_main.index)

    # 2. æ—¥æœŸæ¯”è¾ƒ
    if compare_type == 'date':
        d_main = pd.to_datetime(s_main, errors='coerce')
        d_ref = pd.to_datetime(s_ref, errors='coerce')
        
        # ä»…åœ¨ä¸¤è€…éƒ½æ˜¯æœ‰æ•ˆæ—¥æœŸæ—¶æ¯”è¾ƒ
        valid_dates_mask = d_main.notna() & d_ref.notna()
        date_diff_mask = (d_main.dt.date != d_ref.dt.date)
        errors = valid_dates_mask & date_diff_mask
        
        # å¦‚æœä¸€ä¸ªæ˜¯æ—¥æœŸï¼Œå¦ä¸€ä¸ªä¸æ˜¯ï¼ˆä¸”ä¸ä¸ºç©ºï¼‰ï¼Œä¹Ÿç®—é”™è¯¯
        one_is_date_one_is_not = (d_main.notna() & d_ref.isna() & ~ref_is_na) | \
                                 (d_main.isna() & ~main_is_na & d_ref.notna())
        errors |= one_is_date_one_is_not

    # 3. æ•°å€¼æ¯”è¾ƒ (åŒ…æ‹¬ç‰¹æ®Šçš„ç§ŸèµæœŸé™)
    elif compare_type == 'num':
        s_main_norm = s_main.apply(normalize_num)
        s_ref_norm = s_ref.apply(normalize_num)
        
        # åº”ç”¨ä¹˜æ•°
        if multiplier != 1:
            s_ref_norm = pd.to_numeric(s_ref_norm, errors='coerce') * multiplier
        
        # æ£€æŸ¥æ˜¯å¦éƒ½ä¸ºæ•°å€¼
        is_num_main = s_main_norm.apply(lambda x: isinstance(x, (int, float)))
        is_num_ref = s_ref_norm.apply(lambda x: isinstance(x, (int, float)))
        both_are_num = is_num_main & is_num_ref

        if both_are_num.any():
            diff = (s_main_norm[both_are_num] - s_ref_norm[both_are_num]).abs()
            errors.loc[both_are_num] = (diff > (tolerance + 1e-6)) # 1e-6 é¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜
            
        # å¦‚æœä¸€ä¸ªæ˜¯æ•°å­—ï¼Œå¦ä¸€ä¸ªæ˜¯æ–‡æœ¬ï¼ˆä¸”ä¸ä¸ºç©ºï¼‰ï¼Œä¹Ÿç®—é”™è¯¯
        one_is_num_one_is_not = (is_num_main & ~is_num_ref & ~ref_is_na) | \
                                (~is_num_main & ~main_is_na & is_num_ref)
        errors |= one_is_num_one_is_not

    # 4. æ–‡æœ¬æ¯”è¾ƒ
    else: # compare_type == 'text'
        s_main_norm_text = s_main.apply(normalize_text)
        s_ref_norm_text = s_ref.apply(normalize_text)
        errors = (s_main_norm_text != s_ref_norm_text)

    # 5. æœ€ç»ˆé”™è¯¯é€»è¾‘
    final_errors = errors & ~both_are_na
    
    # æ’é™¤ "Merge å¤±è´¥" å¯¼è‡´çš„é”™è¯¯ (å¤åˆ» 'if ref_rows.empty: return 0')
    lookup_failure_mask = merge_failed_mask & ~main_is_na
    final_errors = final_errors & ~lookup_failure_mask
    
    return final_errors

# ========== æ¯”å¯¹å‡½æ•° ==========
# =====================================
# ğŸ§® å®¡æ ¸å‡½æ•° (å‘é‡åŒ–ç‰ˆ)
# =====================================
def audit_sheet_vec(sheet_name, main_file, all_std_dfs, mapping_rules_vec):
    xls_main = pd.ExcelFile(main_file)
    
    # 1. è¯»å–ä¸»è¡¨ (å°Šé‡åŠ¨æ€è¡¨å¤´)
    header_offset = get_header_row(main_file, sheet_name)
    main_df = pd.read_excel(xls_main, sheet_name=sheet_name, header=header_offset)
    st.write(f"ğŸ“˜ å®¡æ ¸ä¸­ï¼š{sheet_name}ï¼ˆheader={header_offset}ï¼‰")

    contract_col_main = find_col(main_df, "åˆåŒ")
    if not contract_col_main:
        st.error(f"âŒ {sheet_name} ä¸­æœªæ‰¾åˆ°â€œåˆåŒâ€åˆ—ï¼Œå·²è·³è¿‡ã€‚")
        return None, 0

    # 2. å‡†å¤‡ä¸»è¡¨
    main_df['__ROW_IDX__'] = main_df.index
    main_df['__KEY__'] = normalize_contract_key(main_df[contract_col_main])

    # 3. ä¸€æ¬¡æ€§åˆå¹¶æ‰€æœ‰å‚è€ƒæ•°æ®
    merged_df = main_df.copy()
    for std_df in all_std_dfs.values():
        if not std_df.empty:
            merged_df = pd.merge(merged_df, std_df, on='__KEY__', how='left')

    # 4. === éå†å­—æ®µè¿›è¡Œå‘é‡åŒ–æ¯”å¯¹ ===
    total_errors = 0
    errors_locations = set() # å­˜å‚¨ (row_idx, col_name)
    row_has_error = pd.Series(False, index=merged_df.index)

    progress = st.progress(0)
    status = st.empty()
    
    total_comparisons = len(mapping_rules_vec)
    current_comparison = 0

    for main_kw, comparisons in mapping_rules_vec.items():
        current_comparison += 1
        
        main_col = find_col(main_df, main_kw)
        if not main_col:
            continue # è·³è¿‡ä¸»è¡¨ä¸­ä¸å­˜åœ¨çš„åˆ—
        
        status.text(f"æ£€æŸ¥ã€Œ{sheet_name}ã€: {main_kw}...")
        
        # å­˜å‚¨æ­¤å­—æ®µçš„æœ€ç»ˆé”™è¯¯
        field_error_mask = pd.Series(False, index=merged_df.index)
        
        for (ref_col, compare_type, tol, mult) in comparisons:
            if ref_col not in merged_df.columns:
                continue # è·³è¿‡ merge å¤±è´¥æˆ–æœªå®šä¹‰çš„å‚è€ƒåˆ—
            
            s_main = merged_df[main_col]
            s_ref = merged_df[ref_col]
            
            # è·å–æ­¤å•ä¸€æ¯”å¯¹çš„é”™è¯¯
            # (æ³¨æ„ï¼šå¦‚æœä¸€ä¸ªå­—æ®µæœ‰å¤šä¸ªæ¯”å¯¹æº, å®ƒä»¬æ˜¯ 'OR' å…³ç³»)
            # (å³, åªè¦å’Œ *ä¸€ä¸ª* æºåŒ¹é…æˆåŠŸ, å°±ä¸ç®—é”™... 
            #  ...ç­‰ä¸€ä¸‹, åŸå§‹é€»è¾‘æ˜¯ (err1 + err2 + ...), 
            #  è¿™æ„å‘³ç€åªè¦ *ä¸€ä¸ª* æº *ä¸* åŒ¹é…, å°±ç®—é”™)
            
            errors_mask = compare_series_vec(s_main, s_ref, compare_type, tol, mult)
            
            # ç´¯åŠ é”™è¯¯ (åŸå§‹é€»è¾‘æ˜¯ total_errors +=, æ„å‘³ç€ä¸€ä¸ªé”™å°±ç®—é”™)
            field_error_mask |= errors_mask
        
        if field_error_mask.any():
            total_errors += field_error_mask.sum()
            row_has_error |= field_error_mask
            
            # å­˜å‚¨é”™è¯¯ä½ç½® (ä½¿ç”¨ __ROW_IDX__ å’Œ åŸå§‹ main_col åç§°)
            bad_indices = merged_df[field_error_mask]['__ROW_IDX__']
            for idx in bad_indices:
                errors_locations.add((idx, main_col))
        
        progress.progress(current_comparison / total_comparisons)

    status.text(f"ã€Œ{sheet_name}ã€æ¯”å¯¹å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶...")

    # 5. === å¿«é€Ÿå†™å…¥ Excel å¹¶æ ‡æ³¨ ===
    wb = Workbook()
    ws = wb.active
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

   # c. å‡†å¤‡åæ ‡æ˜ å°„ (æˆ‘ä»¬æŠŠ c ç§»åˆ° b ä¹‹å‰)
    original_cols_list = list(main_df.drop(columns=['__ROW_IDX__', '__KEY__']).columns)
    col_name_to_idx = {name: i + 1 for i, name in enumerate(original_cols_list)}
    
    # a. å†™å…¥è¡¨å¤´å‰çš„ç©ºè¡Œ (å¦‚æœéœ€è¦)
    if header_offset > 0:
        for _ in range(header_offset):
            # (ä¿®æ­£ï¼šä½¿ç”¨ original_cols_list çš„é•¿åº¦, è€Œä¸æ˜¯ main_df.columns çš„é•¿åº¦)
            ws.append([""] * len(original_cols_list)) # æ·»åŠ ç©ºè¡Œ
            
    # b. ä½¿ç”¨ dataframe_to_rows å¿«é€Ÿå†™å…¥è¡¨å¤´ + æ•°æ®
    #    (æ³¨æ„ï¼šæˆ‘ä»¬åœ¨è¿™é‡Œä¼ å…¥äº† original_cols_list, ç¡®ä¿åˆ—åºæ­£ç¡®)
    for r in dataframe_to_rows(main_df[original_cols_list], index=False, header=True):
        ws.append(r)

    # d. æ ‡çº¢é”™è¯¯å•å…ƒæ ¼
    for (row_idx, col_name) in errors_locations:
        if col_name in col_name_to_idx:
            excel_row = row_idx + 1 + header_offset + 1 # (row_idx 0-based) + (1 for header) + (offset) + (1 for 1-based)
            excel_col = col_name_to_idx[col_name]
            ws.cell(excel_row, excel_col).fill = red_fill
            
    # e. æ ‡é»„æœ‰é”™è¯¯çš„åˆåŒå·
    if contract_col_main in col_name_to_idx:
        contract_col_excel_idx = col_name_to_idx[contract_col_main]
        error_row_indices = merged_df[row_has_error]['__ROW_IDX__']
        for row_idx in error_row_indices:
            excel_row = row_idx + 1 + header_offset + 1
            ws.cell(excel_row, contract_col_excel_idx).fill = yellow_fill

    # 6. å¯¼å‡º
    output_stream = BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è½½ {sheet_name} å®¡æ ¸æ ‡æ³¨ç‰ˆ",
        data=output_stream,
        file_name=f"{sheet_name}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key=f"download_{sheet_name}" # æ·»åŠ å”¯ä¸€çš„key
    )

    # --- VVVV (ã€æ–°åŠŸèƒ½ã€‘ä»è¿™é‡Œå¼€å§‹æ·»åŠ ) VVVV ---

    # 7. (æ–°) å¯¼å‡ºä»…å«é”™è¯¯è¡Œçš„æ–‡ä»¶ (å¸¦æ ‡çº¢)
    if row_has_error.any():
        try:
            # 1. è·å–ä»…å«é”™è¯¯è¡Œçš„ DataFrame (åªä¿ç•™åŸå§‹åˆ—)
            df_errors_only = merged_df.loc[row_has_error, original_cols_list].copy()
            
            # 2. å…³é”®ï¼šåˆ›å»º "åŸå§‹è¡Œç´¢å¼•" åˆ° "æ–°Excelè¡Œå·" çš„æ˜ å°„
            #    æˆ‘ä»¬è·å–æ‰€æœ‰å‡ºé”™è¡Œçš„ __ROW_IDX__
            original_indices_with_error = merged_df.loc[row_has_error, '__ROW_IDX__']
            
            #    åˆ›å»ºæ˜ å°„: { åŸå§‹ç´¢å¼• : æ–°çš„Excelè¡Œå· }
            #    (enumerate start=2, å› ä¸º Excel è¡Œ 1 æ˜¯è¡¨å¤´, æ•°æ®ä»è¡Œ 2 å¼€å§‹)
            original_idx_to_new_excel_row = {
                original_idx: new_row_num 
                for new_row_num, original_idx in enumerate(original_indices_with_error, start=2)
            }

            # 3. åˆ›å»ºä¸€ä¸ªæ–°çš„å·¥ä½œç°¿(Workbook)
            wb_errors = Workbook()
            ws_errors = wb_errors.active
            
            # 4. ä½¿ç”¨ dataframe_to_rows å¿«é€Ÿå†™å…¥æ•°æ®
            for r in dataframe_to_rows(df_errors_only, index=False, header=True):
                ws_errors.append(r)
                
            # 5. éå†ä¸»é”™è¯¯åˆ—è¡¨(errors_locations)ï¼Œè¿›è¡Œæ ‡çº¢
            #    (col_name_to_idx å’Œ red_fill å·²åœ¨å‰é¢å®šä¹‰)
            for (original_row_idx, col_name) in errors_locations:
                
                # æ£€æŸ¥è¿™ä¸ªé”™è¯¯æ˜¯å¦åœ¨æˆ‘ä»¬ "ä»…é”™è¯¯è¡Œ" çš„æ˜ å°„ä¸­
                if original_row_idx in original_idx_to_new_excel_row:
                    
                    # è·å–å®ƒåœ¨æ–°Excelæ–‡ä»¶ä¸­çš„è¡Œå·
                    new_row = original_idx_to_new_excel_row[original_row_idx]
                    
                    # è·å–åˆ—å·
                    if col_name in col_name_to_idx:
                        new_col = col_name_to_idx[col_name]
                        
                        # åº”ç”¨æ ‡çº¢
                        ws_errors.cell(row=new_row, column=new_col).fill = red_fill
            
            # 6. ä¿å­˜åˆ° BytesIO
            output_errors_only = BytesIO()
            wb_errors.save(output_errors_only)
            output_errors_only.seek(0)
            
            # 7. åˆ›å»ºç¬¬äºŒä¸ªä¸‹è½½æŒ‰é’®
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½ {sheet_name} (ä»…å«é”™è¯¯è¡Œ, å¸¦æ ‡çº¢)", # æ›´æ–°äº†æ ‡ç­¾
                data=output_errors_only,
                file_name=f"{sheet_name}_ä»…é”™è¯¯è¡Œ_æ ‡çº¢.xlsx", # æ›´æ–°äº†æ–‡ä»¶å
                key=f"download_{sheet_name}_errors_only" # å¿…é¡»ä½¿ç”¨å”¯ä¸€çš„ key
            )
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆâ€œä»…é”™è¯¯è¡Œâ€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
    # --- ^^^^ (ã€æ–°åŠŸèƒ½ã€‘åˆ°è¿™é‡Œç»“æŸ) ^^^^ ---
    st.success(f"âœ… {sheet_name} å®¡æ ¸å®Œæˆï¼Œå…±å‘ç° {total_errors} å¤„é”™è¯¯")
    
    # è¿”å›åŸå§‹çš„ main_df (ä¸å« __KEY__), ç”¨äºæ¼å¡«æ£€æµ‹
    return main_df.drop(columns=['__ROW_IDX__', '__KEY__']), total_errors

# ========== æ–‡ä»¶è¯»å– & é¢„å¤„ç† ==========
main_file = find_file(uploaded_files, "ææˆé¡¹ç›®")
ec_file = find_file(uploaded_files, "äºŒæ¬¡æ˜ç»†")
fk_file = find_file(uploaded_files, "æ”¾æ¬¾æ˜ç»†")
product_file = find_file(uploaded_files, "äº§å“å°è´¦")
overdue_file = find_file(uploaded_files, "è¶…æœŸæ˜ç»†")

st.info("â„¹ï¸ æ­£åœ¨è¯»å–å¹¶é¢„å¤„ç†å‚è€ƒæ–‡ä»¶...")

# 1. åŠ è½½æ‰€æœ‰å‚è€ƒ DF
ec_df = pd.read_excel(ec_file)
fk_xls = pd.ExcelFile(fk_file)
fk_df = pd.read_excel(fk_xls, sheet_name=[s for s in fk_xls.sheet_names if "æœ¬å¸" in s][0])
product_df = pd.read_excel(product_file)
overdue_df = pd.read_excel(overdue_file)

# ---- æ–°å¢ææˆsheetæå– ----
commission_sheets = [s for s in fk_xls.sheet_names if "ææˆ" in s]
commission_df = pd.read_excel(fk_xls, sheet_name=commission_sheets[0]) if commission_sheets else None

# ---- æ‰¾åˆ°æ‰€æœ‰å‚è€ƒè¡¨çš„åˆåŒåˆ— ----
contract_col_ec = find_col(ec_df, "åˆåŒ")
contract_col_fk = find_col(fk_df, "åˆåŒ")
contract_col_comm = find_col(commission_df, "åˆåŒ") if commission_df is not None else None
contract_col_product = find_col(product_df, "åˆåŒ")
contract_col_overdue = find_col(overdue_df, "åˆåŒ")

# 2. (æ–°) å®šä¹‰å‘é‡åŒ–æ˜ å°„è§„åˆ™
# æ ¼å¼: { "ä¸»è¡¨åˆ—å": [ (å‚è€ƒåˆ—è¡¨å, æ¯”è¾ƒç±»å‹, å®¹å·®, ä¹˜æ•°), ... ] }
mapping_rules_vec = {
    "èµ·ç§Ÿæ—¥æœŸ": [
        ("ref_ec_èµ·ç§Ÿæ—¥_å•†", 'date', 0, 1),
        ("ref_product_èµ·ç§Ÿæ—¥_å•†", 'date', 0, 1)
    ],
    "ç§Ÿèµæœ¬é‡‘": [("ref_fk_ç§Ÿèµæœ¬é‡‘", 'num', 0, 1)],
    "æ”¶ç›Šç‡": [("ref_product_XIRR_å•†_èµ·ç§Ÿ", 'num', 0.005, 1)],
    "æ“ä½œäºº": [("ref_fk_å®¢æˆ·ç»ç†", 'text', 0, 1)],
    "å®¢æˆ·ç»ç†": [("ref_fk_å®¢æˆ·ç»ç†", 'text', 0, 1)],
    "äº§å“": [("ref_product_äº§å“åç§°_å•†", 'text', 0, 1)],
    "åŸå¸‚ç»ç†": [("ref_overdue_åŸå¸‚ç»ç†", 'text', 0, 1)],
}

# 3. (æ–°) é¢„å¤„ç†æ‰€æœ‰å‚è€ƒ DF
# ä» mapping_rules_vec ä¸­æå–æ‰€æœ‰éœ€è¦çš„åˆ—
ec_cols = ["èµ·ç§Ÿæ—¥_å•†"]
fk_cols = ["ç§Ÿèµæœ¬é‡‘", "å®¢æˆ·ç»ç†"]
product_cols = ["èµ·ç§Ÿæ—¥_å•†", "XIRR_å•†_èµ·ç§Ÿ", "äº§å“åç§°_å•†"]
overdue_cols = ["åŸå¸‚ç»ç†"]

ec_std = prepare_one_ref_df(ec_df, contract_col_ec, ec_cols, "ec")
fk_std = prepare_one_ref_df(fk_df, contract_col_fk, fk_cols, "fk")
product_std = prepare_one_ref_df(product_df, contract_col_product, product_cols, "product")
overdue_std = prepare_one_ref_df(overdue_df, contract_col_overdue, overdue_cols, "overdue")

all_std_dfs = {
    "ec": ec_std,
    "fk": fk_std,
    "product": product_std,
    "overdue": overdue_std
}

st.success("âœ… å‚è€ƒæ–‡ä»¶é¢„å¤„ç†å®Œæˆã€‚")

# ========== æ‰§è¡Œä¸»æµç¨‹ (å‘é‡åŒ–) ==========
xls_main = pd.ExcelFile(main_file)
target_sheets = [
    s for s in xls_main.sheet_names
    if any(k in s for k in ["èµ·ç§Ÿ", "äºŒæ¬¡", "å¹³å°å·¥", "ç‹¬ç«‹æ¶æ„", "ä½ä»·å€¼", "æƒè´£å‘ç”Ÿ"])
]

all_contracts_in_sheets = set()

if not target_sheets:
    st.warning("âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡ sheetã€‚")
else:
    for sheet_name in target_sheets:
        # (æ–°) è°ƒç”¨å‘é‡åŒ–å®¡æ ¸å‡½æ•°
        df, _ = audit_sheet_vec(sheet_name, main_file, all_std_dfs, mapping_rules_vec)
        
        if df is not None:
            col = find_col(df, "åˆåŒ")
            if col:
                # (æ–°) æ ‡å‡†åŒ–åˆåŒå·, ç”¨äº set.update
                normalized_contracts = normalize_contract_key(df[col].dropna())
                all_contracts_in_sheets.update(normalized_contracts)

# ======= æ–°é€»è¾‘ï¼šä½¿ç”¨â€œææˆâ€sheetåˆåŒå·æ£€æµ‹æ¼å¡« =======
if commission_df is not None and contract_col_comm:
    # (æ–°) å¿…é¡»åŒæ ·æ ‡å‡†åŒ–ææˆè¡¨çš„åˆåŒå·
    commission_contracts = set(normalize_contract_key(commission_df[contract_col_comm].dropna()))
    
    missing_contracts = sorted(list(commission_contracts - all_contracts_in_sheets))

    # --- VVVV (ä»è¿™é‡Œå¼€å§‹, ä¿®å¤äº†ç¼©è¿›) VVVV ---
    st.subheader("ğŸ“‹ åˆåŒæ¼å¡«æ£€æµ‹ç»“æœï¼ˆåŸºäºææˆsheetï¼‰")
    st.write(f"å…± {len(missing_contracts)} ä¸ªåˆåŒåœ¨å…­å¼ è¡¨ä¸­æœªå‡ºç°ã€‚")

    if missing_contracts:
        wb_miss = Workbook()
        ws_miss = wb_miss.active
        ws_miss.cell(1, 1, "æœªå‡ºç°åœ¨ä»»ä¸€è¡¨ä¸­çš„åˆåŒå·")
        yellow = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        for i, cno in enumerate(missing_contracts, start=2):
            ws_miss.cell(i, 1, cno).fill = yellow

        out_miss = BytesIO()
        wb_miss.save(out_miss)
        out_miss.seek(0)
        st.download_button(
            "ğŸ“¥ ä¸‹è½½æ¼å¡«åˆåŒåˆ—è¡¨",
            data=out_miss,
            file_name="æ¼å¡«åˆåŒå·åˆ—è¡¨.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.success("âœ… æ‰€æœ‰ææˆsheetåˆåŒå·å‡å·²å‡ºç°åœ¨å…­å¼ è¡¨ä¸­ï¼Œæ— æ¼å¡«ã€‚")
_ # --- ^^^^ (åˆ°è¿™é‡Œç»“æŸ) ^^^^ ---
