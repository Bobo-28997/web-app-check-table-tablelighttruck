# =====================================
# Streamlit App: äººäº‹ç”¨â€œé¡¹ç›®ææˆ & äºŒæ¬¡é¡¹ç›® & å¹³å°å·¥ & ç‹¬ç«‹æ¶æ„ & ä½ä»·å€¼â€è‡ªåŠ¨å®¡æ ¸
# (V3: ç¼“å­˜ä¼˜åŒ–ç‰ˆ)
# =====================================

import streamlit as st
import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
from io import BytesIO
import unicodedata, re
import time 

# =====================================
# ğŸ§° å·¥å…·å‡½æ•° (ä¸å˜)
# =====================================

def find_file(files_list, keyword):
    for f in files_list:
        if keyword in f.name:
            return f
    raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„æ–‡ä»¶")

def normalize_colname(c):
    return str(c).strip().lower()

def find_col(df, keyword, exact=False):
    if df is None:
        return None
    key = keyword.strip().lower()
    for col in df.columns:
        cname = normalize_colname(col)
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

def normalize_num(val):
    if pd.isna(val):
        return None
    s = str(val).replace(",", "").strip()
    if s in ["", "-", "nan"]:
        return None
    try:
        if "%" in s:
            s = s.replace("%", "")
            return float(s) / 100
        return float(s)
    except:
        return s

def normalize_text(val):
    if pd.isna(val):
        return ""
    s = str(val)
    s = re.sub(r'[\n\r\t ]+', '', s)
    s = s.replace('\u3000', '')
    s = ''.join(unicodedata.normalize('NFKC', ch) for ch in s)
    return s.lower().strip()

def detect_header_row(file, sheet_name):
    # (æ³¨æ„: file åœ¨ç¼“å­˜å‡½æ•°ä¸­æ˜¯ UploadedFile å¯¹è±¡)
    preview = pd.read_excel(file, sheet_name=sheet_name, nrows=2, header=None)
    first_row = preview.iloc[0]
    total_cells = len(first_row)
    empty_like = sum(
        (pd.isna(x) or str(x).startswith("Unnamed") or str(x).strip() == "")
        for x in first_row
    )
    empty_ratio = empty_like / total_cells if total_cells > 0 else 0
    return 1 if empty_ratio >= 0.7 else 0

def get_header_row(file, sheet_name):
    if any(k in sheet_name for k in ["èµ·ç§Ÿ", "äºŒæ¬¡"]):
        return 1
    return detect_header_row(file, sheet_name)

def normalize_contract_key(series: pd.Series) -> pd.Series:
    s = series.astype(str)
    s = s.str.replace(r"\.0$", "", regex=True) 
    s = s.str.strip()
    s = s.str.upper() 
    s = s.str.replace('ï¼', '-', regex=False)
    return s

def prepare_one_ref_df(ref_df, ref_contract_col, required_cols, prefix):
    if ref_df is None:
        st.warning(f"âš ï¸ å‚è€ƒæ–‡ä»¶ '{prefix}' æœªåŠ è½½ (df is None)ã€‚")
        return pd.DataFrame(columns=['__KEY__'])
        
    if ref_contract_col is None:
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°'åˆåŒ'åˆ—ï¼Œè·³è¿‡æ­¤æ•°æ®æºã€‚")
        return pd.DataFrame(columns=['__KEY__'])

    cols_to_extract = []
    col_mapping = {} 

    for col_kw in required_cols:
        actual_col = find_col(ref_df, col_kw)
        
        if actual_col:
            cols_to_extract.append(actual_col)
            col_mapping[actual_col] = f"ref_{prefix}_{col_kw}"
        else:
            st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°åˆ— (å…³é”®å­—: '{col_kw}')")
            
    if not cols_to_extract:
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°ä»»ä½•æ‰€éœ€å­—æ®µï¼Œè·³è¿‡ã€‚")
        return pd.DataFrame(columns=['__KEY__'])

    cols_to_extract.append(ref_contract_col)
    cols_to_extract_unique = list(set(cols_to_extract))
    valid_cols = [col for col in cols_to_extract_unique if col in ref_df.columns]
    std_df = ref_df[valid_cols].copy()
    
    std_df['__KEY__'] = normalize_contract_key(std_df[ref_contract_col])
    std_df = std_df.rename(columns=col_mapping)
    final_cols = ['__KEY__'] + list(col_mapping.values())
    final_cols_in_df = [col for col in final_cols if col in std_df.columns]
    std_df = std_df[final_cols_in_df]
    std_df = std_df.drop_duplicates(subset=['__KEY__'], keep='first')
    return std_df

def compare_series_vec(s_main, s_ref, compare_type='text', tolerance=0, multiplier=1):
    merge_failed_mask = s_ref.isna()
    main_is_na = pd.isna(s_main) | (s_main.astype(str).str.strip().isin(["", "nan", "None"]))
    ref_is_na = pd.isna(s_ref) | (s_ref.astype(str).str.strip().isin(["", "nan", "None"]))
    both_are_na = main_is_na & ref_is_na
    
    errors = pd.Series(False, index=s_main.index)

    if compare_type == 'date':
        d_main = pd.to_datetime(s_main, errors='coerce')
        d_ref = pd.to_datetime(s_ref, errors='coerce')
        
        valid_dates_mask = d_main.notna() & d_ref.notna()
        date_diff_mask = (d_main.dt.date != d_ref.dt.date)
        errors = valid_dates_mask & date_diff_mask
        
        one_is_date_one_is_not = (d_main.notna() & d_ref.isna() & ~ref_is_na) | \
                                 (d_main.isna() & ~main_is_na & d_ref.notna())
        errors |= one_is_date_one_is_not

    elif compare_type in ['num', 'num_term']:
        s_main_norm = s_main.apply(normalize_num)
        s_ref_norm = s_ref.apply(normalize_num)
        
        is_num_main = s_main_norm.apply(lambda x: isinstance(x, (int, float)))
        is_num_ref = s_ref_norm.apply(lambda x: isinstance(x, (int, float)))
        both_are_num = is_num_main & is_num_ref

        if both_are_num.any():
            diff = (s_main_norm[both_are_num] - s_ref_norm[both_are_num]).abs()
            
            if compare_type == 'num_term':
                errors.loc[both_are_num] = (diff >= 1.0)
            else:
                errors.loc[both_are_num] = (diff > (tolerance + 1e-6))
                
        one_is_num_one_is_not = (is_num_main & ~is_num_ref & ~ref_is_na) | \
                                (~is_num_main & ~main_is_na & is_num_ref)
        errors |= one_is_num_one_is_not

    else: 
        s_main_norm_text = s_main.apply(normalize_text)
        s_ref_norm_text = s_ref.apply(normalize_text)
        errors = (s_main_norm_text != s_ref_norm_text)

    final_errors = errors & ~both_are_na
    lookup_failure_mask = merge_failed_mask & ~main_is_na
    final_errors = final_errors & ~lookup_failure_mask
    
    return final_errors

# =====================================
# ğŸ§® (ä¿®æ”¹) å®¡æ ¸å‡½æ•° - ç°åœ¨è¿”å›æ–‡ä»¶
# =====================================
def audit_sheet_vec(sheet_name, main_file, all_std_dfs, mapping_rules_vec):
    xls_main = pd.ExcelFile(main_file)
    
    # (æ³¨æ„: get_header_row æ¥æ”¶çš„æ˜¯ UploadedFile å¯¹è±¡, ä¸æ˜¯è·¯å¾„)
    header_offset = get_header_row(main_file, sheet_name)
    main_df = pd.read_excel(xls_main, sheet_name=sheet_name, header=header_offset)
    st.write(f"ğŸ“˜ å®¡æ ¸ä¸­ï¼š{sheet_name}ï¼ˆheader={header_offset}ï¼‰")

    contract_col_main = find_col(main_df, "åˆåŒ")
    if not contract_col_main:
        st.error(f"âŒ {sheet_name} ä¸­æœªæ‰¾åˆ°â€œåˆåŒâ€åˆ—ï¼Œå·²è·³è¿‡ã€‚")
        return None, 0, {} # (è¿”å› df, total_errors, files_dict)

    main_df['__ROW_IDX__'] = main_df.index
    main_df['__KEY__'] = normalize_contract_key(main_df[contract_col_main])

    merged_df = main_df.copy()
    for std_df in all_std_dfs.values():
        if not std_df.empty:
            merged_df = pd.merge(merged_df, std_df, on='__KEY__', how='left')

    total_errors = 0
    errors_locations = set()
    row_has_error = pd.Series(False, index=merged_df.index)

    progress = st.progress(0)
    status = st.empty()
    
    total_comparisons = len(mapping_rules_vec)
    current_comparison = 0

    for main_kw, comparisons in mapping_rules_vec.items():
        current_comparison += 1
        
        main_col = find_col(main_df, main_kw)
        if not main_col:
            continue 
        
        status.text(f"æ£€æŸ¥ã€Œ{sheet_name}ã€: {main_kw}...")
        
        field_error_mask = pd.Series(False, index=merged_df.index)
        
        for (ref_col, compare_type, tol, mult) in comparisons:
            if ref_col not in merged_df.columns:
                continue 
            
            s_main = merged_df[main_col]
            s_ref = merged_df[ref_col]

            skip_mask = pd.Series(False, index=merged_df.index) 
            
            if main_kw == "åŸå¸‚ç»ç†":
                na_mask = pd.isna(s_ref)
                str_val = s_ref.astype(str).str.strip().str.lower()
                str_mask = str_val.isin(["", "nan", "none", "null", "0", "0.0"])
                skip_mask = na_mask | str_mask
            
            errors_mask = compare_series_vec(s_main, s_ref, compare_type, tol, mult)
            final_errors_mask = errors_mask & ~skip_mask
            field_error_mask |= final_errors_mask
        
        if field_error_mask.any():
            total_errors += field_error_mask.sum()
            row_has_error |= field_error_mask
            
            bad_indices = merged_df[field_error_mask]['__ROW_IDX__']
            for idx in bad_indices:
                errors_locations.add((idx, main_col))
        
        progress.progress(current_comparison / total_comparisons)

    status.text(f"ã€Œ{sheet_name}ã€æ¯”å¯¹å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶...")

    # 5. === å¿«é€Ÿå†™å…¥ Excel å¹¶æ ‡æ³¨ ===
    wb = Workbook()
    ws = wb.active
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    original_cols_list = list(main_df.drop(columns=['__ROW_IDX__', '__KEY__']).columns)
    col_name_to_idx = {name: i + 1 for i, name in enumerate(original_cols_list)}
    
    if header_offset > 0:
        for _ in range(header_offset):
            ws.append([""] * len(original_cols_list))
            
    for r in dataframe_to_rows(main_df[original_cols_list], index=False, header=True):
        ws.append(r)

    for (row_idx, col_name) in errors_locations:
        if col_name in col_name_to_idx:
            excel_row = row_idx + 1 + header_offset + 1
            excel_col = col_name_to_idx[col_name]
            ws.cell(excel_row, excel_col).fill = red_fill
            
    if contract_col_main in col_name_to_idx:
        contract_col_excel_idx = col_name_to_idx[contract_col_main]
        error_row_indices = merged_df[row_has_error]['__ROW_IDX__']
        for row_idx in error_row_indices:
            excel_row = row_idx + 1 + header_offset + 1
            ws.cell(excel_row, contract_col_excel_idx).fill = yellow_fill

    # 6. (ä¿®æ”¹) å¯¼å‡ºåˆ° BytesIO å¹¶å‡†å¤‡è¿”å›
    output_stream = BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    
    files_to_save = {
        "full_report": (f"{sheet_name}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx", output_stream),
        "error_report": (None, None)
    }

    # 7. (ä¿®æ”¹) å¯¼å‡ºä»…é”™è¯¯è¡Œåˆ° BytesIO
    if row_has_error.any():
        try:
            df_errors_only = merged_df.loc[row_has_error, original_cols_list].copy()
            original_indices_with_error = merged_df.loc[row_has_error, '__ROW_IDX__']
            original_idx_to_new_excel_row = {
                original_idx: new_row_num 
                for new_row_num, original_idx in enumerate(original_indices_with_error, start=2)
            }
            wb_errors = Workbook()
            ws_errors = wb_errors.active
            for r in dataframe_to_rows(df_errors_only, index=False, header=True):
                ws_errors.append(r)
            for (original_row_idx, col_name) in errors_locations:
                if original_row_idx in original_idx_to_new_excel_row:
                    new_row = original_idx_to_new_excel_row[original_row_idx]
                    if col_name in col_name_to_idx:
                        new_col = col_name_to_idx[col_name]
                        ws_errors.cell(row=new_row, column=new_col).fill = red_fill
            
            output_errors_only = BytesIO()
            wb_errors.save(output_errors_only)
            output_errors_only.seek(0)
            
            files_to_save["error_report"] = (f"{sheet_name}_ä»…é”™è¯¯è¡Œ_æ ‡çº¢.xlsx", output_errors_only)
            
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆâ€œä»…é”™è¯¯è¡Œâ€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
    st.success(f"âœ… {sheet_name} å®¡æ ¸å®Œæˆï¼Œå…±å‘ç° {total_errors} å¤„é”™è¯¯")
    
    # (ä¿®æ”¹) è¿”å› df, total_errors, å’Œ files_dict
    return main_df.drop(columns=['__ROW_IDX__', '__KEY__']), total_errors, files_to_save

# =====================================
# ğŸ•µï¸ (æ–°) æ¼å¡«æ£€æŸ¥å‡½æ•°
# =====================================
def run_leaky_check(commission_df, contract_col_comm, all_contracts_in_sheets):
    """
    æ‰§è¡Œæ¼å¡«æ£€æŸ¥å¹¶è¿”å› BytesIO æ–‡ä»¶ã€‚
    """
    st.subheader("ğŸ“‹ åˆåŒæ¼å¡«æ£€æµ‹ç»“æœï¼ˆåŸºäºææˆsheetï¼‰")
    files_to_save = {}
    
    if commission_df is None or contract_col_comm is None:
        st.warning("âš ï¸ æœªåŠ è½½â€œææˆâ€sheetæˆ–æœªæ‰¾åˆ°åˆåŒåˆ—ï¼Œè·³è¿‡æ¼å¡«æ£€æŸ¥ã€‚")
        return 0, {}

    commission_contracts = set(normalize_contract_key(commission_df[contract_col_comm].dropna()))
    missing_contracts = sorted(list(commission_contracts - all_contracts_in_sheets))
    æ¼å¡«åˆåŒæ•° = len(missing_contracts)

    st.write(f"å…± {æ¼å¡«åˆåŒæ•°} ä¸ªåˆåŒåœ¨æ‰€æœ‰æ£€æŸ¥è¡¨ä¸­æœªå‡ºç°ã€‚")

    if missing_contracts:
        wb_miss = Workbook()
        ws_miss = wb_miss.active
        ws_miss.cell(1, 1, "æœªå‡ºç°åœ¨ä»»ä¸€è¡¨ä¸­çš„åˆåŒå·")
        yellow = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        for i, cno in enumerate(missing_contracts, start=2):
            ws_miss.cell(i, 1, cno).fill = yellow

        out_miss = BytesIO()
        wb_miss.save(out_miss)
        out_miss.seek(0)
        
        files_to_save["leaky_list"] = ("æ¼å¡«åˆåŒå·åˆ—è¡¨.xlsx", out_miss)
    
    else:
        st.success("âœ… æ‰€æœ‰ææˆsheetåˆåŒå·å‡å·²å‡ºç°åœ¨æ£€æŸ¥è¡¨ä¸­ï¼Œæ— æ¼å¡«ã€‚")
        
    return æ¼å¡«åˆåŒæ•°, files_to_save

# =====================================
# ğŸš€ (æ–°) ç¼“å­˜çš„å®¡æ ¸ä¸»å‡½æ•°
# =====================================
@st.cache_data(show_spinner="æ­£åœ¨æ‰§è¡Œå®¡æ ¸ï¼Œè¯·ç¨å€™...")
def run_full_audit(_uploaded_files):
    """
    æ‰§è¡Œæ‰€æœ‰æ–‡ä»¶è¯»å–ã€é¢„å¤„ç†å’Œæ£€æŸ¥ï¼Œå¹¶è¿”å›æ‰€æœ‰ç»“æœã€‚
    """
    
    # --- 1. ğŸ“– æ–‡ä»¶è¯»å– & é¢„å¤„ç† ---
    main_file = find_file(_uploaded_files, "é¡¹ç›®ææˆ")
    ec_file = find_file(_uploaded_files, "äºŒæ¬¡æ˜ç»†")
    fk_file = find_file(_uploaded_files, "æ”¾æ¬¾æ˜ç»†")
    product_file = find_file(_uploaded_files, "äº§å“å°è´¦")

    st.info("â„¹ï¸ æ­£åœ¨è¯»å–å¹¶é¢„å¤„ç†å‚è€ƒæ–‡ä»¶...")

    ec_df = pd.read_excel(ec_file)
    product_df = pd.read_excel(product_file)
    fk_xls = pd.ExcelFile(fk_file)

    commission_sheets = [s for s in fk_xls.sheet_names if "ææˆ" in s]
    if not commission_sheets:
        st.error("âŒ åœ¨ 'æ”¾æ¬¾æ˜æ˜ç»†' æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½•åŒ…å« 'ææˆ' çš„sheetï¼ç¨‹åºå·²åœæ­¢ã€‚")
        st.stop()
    st.info(f"â„¹ï¸ æ­£åœ¨ä» 'æ”¾æ¬¾æ˜ç»†' åŠ è½½ {len(commission_sheets)} ä¸ª 'ææˆ' sheet...")
    commission_df_list = [pd.read_excel(fk_xls, sheet_name=s) for s in commission_sheets]
    fk_commission_df = pd.concat(commission_df_list, ignore_index=True)
    
    fk_df = fk_commission_df
    commission_df = fk_commission_df

    contract_col_ec = find_col(ec_df, "åˆåŒ")
    contract_col_fk = find_col(fk_df, "åˆåŒ")
    contract_col_comm = find_col(commission_df, "åˆåŒ")
    contract_col_product = find_col(product_df, "åˆåŒ")

    # --- 2. ğŸ—ºï¸ æ˜ å°„è¡¨ ---
    mapping_rules_vec = {
        "èµ·ç§Ÿæ—¥æœŸ": [("ref_ec_èµ·ç§Ÿæ—¥_å•†", 'date', 0, 1)],
        "ç§Ÿèµæœ¬é‡‘": [("ref_fk_ç§Ÿèµæœ¬é‡‘", 'num', 0, 1)],
        "æ”¶ç›Šç‡": [("ref_product_XIRR_å•†_èµ·ç§Ÿ", 'num', 0.005, 1)],
        "æ“ä½œäºº": [("ref_fk_ææŠ¥äººå‘˜", 'text', 0, 1)],
        "å®¢æˆ·ç»ç†": [("ref_fk_ææŠ¥äººå‘˜", 'text', 0, 1)],
        "åŸå¸‚ç»ç†": [("ref_fk_åŸå¸‚ç»ç†", 'text', 0, 1)],
        "å®ŒæˆäºŒæ¬¡äº¤æ¥æ—¶é—´": [("ref_ec_å‡ºæœ¬æµç¨‹æ—¶é—´", 'date', 0, 1)],
        "å¹´åŒ–MIN": [("ref_product_XIRR_å•†_èµ·ç§Ÿ", 'num', 0.005, 1)],
        "å¹´é™": [("ref_fk_ç§ŸèµæœŸé™", 'num_term', 0, 0)]
    }

    # --- 3. ğŸš€ é¢„å¤„ç† (æå–åˆ—) ---
    ec_cols = ["èµ·ç§Ÿæ—¥_å•†", "å‡ºæœ¬æµç¨‹æ—¶é—´"]
    fk_cols = ["ç§Ÿèµæœ¬é‡‘", "ææŠ¥äººå‘˜", "åŸå¸‚ç»ç†", "ç§ŸèµæœŸé™"]
    product_cols = ["èµ·ç§Ÿæ—¥_å•†", "XIRR_å•†_èµ·ç§Ÿ"]

    ec_std = prepare_one_ref_df(ec_df, contract_col_ec, ec_cols, "ec")
    fk_std = prepare_one_ref_df(fk_df, contract_col_fk, fk_cols, "fk")
    product_std = prepare_one_ref_df(product_df, contract_col_product, product_cols, "product")

    all_std_dfs = {
        "ec": ec_std,
        "fk": fk_std,
        "product": product_std,
    }
    st.success("âœ… å‚è€ƒæ–‡ä»¶é¢„å¤„ç†å®Œæˆã€‚")

    # --- 4. ğŸ§¾ å¤šsheetå¾ªç¯ ---
    xls_main = pd.ExcelFile(main_file)
    target_sheets = [
        s for s in xls_main.sheet_names
        if any(k in s for k in ["èµ·ç§Ÿ", "äºŒæ¬¡", "å¹³å°å·¥", "ç‹¬ç«‹æ¶æ„", "ä½ä»·å€¼"])
    ]
    
    all_contracts_in_sheets = set()
    total_errors_all_sheets = 0
    all_generated_files = [] # å­˜å‚¨æ‰€æœ‰ (æ–‡ä»¶å, BytesIO) å…ƒç»„

    if not target_sheets:
        st.warning("âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡ sheetã€‚")
    else:
        for sheet_name in target_sheets:
            df, total_errors, files_dict = audit_sheet_vec(sheet_name, main_file, all_std_dfs, mapping_rules_vec)
            
            # æ”¶é›†æ–‡ä»¶
            all_generated_files.append(files_dict["full_report"])
            if files_dict["error_report"][0] is not None:
                all_generated_files.append(files_dict["error_report"])
            
            total_errors_all_sheets += total_errors
            
            if df is not None:
                col = find_col(df, "åˆåŒ")
                if col:
                    normalized_contracts = normalize_contract_key(df[col].dropna())
                    all_contracts_in_sheets.update(normalized_contracts)

    # --- 5. ğŸ•µï¸ æ¼å¡«æ£€æŸ¥ ---
    æ¼å¡«åˆåŒæ•°, leaky_files_dict = run_leaky_check(
        commission_df, contract_col_comm, all_contracts_in_sheets
    )
    
    if "leaky_list" in leaky_files_dict:
        all_generated_files.append(leaky_files_dict["leaky_list"])

    # --- 6. è¿”å›æ‰€æœ‰ç»“æœ ---
    stats_summary = {
        "total_errors": total_errors_all_sheets,
        "leaky_count": æ¼å¡«åˆåŒæ•°
    }
    
    return all_generated_files, stats_summary

# =====================================
# ğŸ åº”ç”¨æ ‡é¢˜ä¸è¯´æ˜ (é‡æ„ç‰ˆ)
# =====================================
st.title("ğŸ“Š æ¨¡æ‹Ÿäººäº‹ç”¨è–ªèµ„è®¡ç®—è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿ-2 (æ–°è§„åˆ™ç‰ˆ)")
st.image("image/app2.png")

# =====================================
# ğŸ“‚ ä¸Šä¼ æ–‡ä»¶åŒºï¼šè¦æ±‚ä¸Šä¼  4 ä¸ª xlsx æ–‡ä»¶ (é‡æ„ç‰ˆ)
# =====================================
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ æ–‡ä»¶åä¸­åŒ…å«ä»¥ä¸‹å­—æ®µçš„æ–‡ä»¶ï¼šé¡¹ç›®ææˆã€æ”¾æ¬¾æ˜ç»†ã€äºŒæ¬¡æ˜ç»†ã€äº§å“å°è´¦ã€‚æœ€åèªŠå†™ï¼Œéœ€æ£€çš„è¡¨ä¸ºé¡¹ç›®ææˆè¡¨ã€‚",
    type="xlsx",
    accept_multiple_files=True
)

# =====================================
# ğŸš€ (æ–°) ä¸»æ‰§è¡Œé€»è¾‘
# =====================================
if not uploaded_files or len(uploaded_files) < 4:
    st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ 4 ä¸ªæ–‡ä»¶ï¼ˆé¡¹ç›®ææˆã€äºŒæ¬¡æ˜ç»†ã€æ”¾æ¬¾æ˜ç»†ã€äº§å“å°è´¦ï¼‰")
    st.stop()
else:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
    
    # (æ–°) â€œå¼€å§‹å®¡æ ¸â€æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹å®¡æ ¸", type="primary"):
        # å°†è¿è¡ŒçŠ¶æ€å­˜å…¥ session state
        st.session_state.audit_run = True
    
    # (æ–°) â€œé‡æ–°å®¡æ ¸â€æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤ç¼“å­˜
    if st.button("ğŸ”„ æ¸…é™¤ç¼“å­˜å¹¶é‡æ–°å®¡æ ¸"):
        run_full_audit.clear()
        st.session_state.audit_run = True
        st.rerun()

    # (æ–°) åªæœ‰åœ¨ "å¼€å§‹å®¡æ ¸" è¢«ç‚¹å‡»åæ‰æ‰§è¡Œ
    if 'audit_run' in st.session_state and st.session_state.audit_run:
        try:
            # 1. (æ–°) è°ƒç”¨ç¼“å­˜çš„å®¡æ ¸å‡½æ•°
            all_files, stats = run_full_audit(uploaded_files)

            # 2. (æ–°) æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
            st.success(f"ğŸ¯ å…¨éƒ¨å®¡æ ¸å®Œæˆï¼Œå…± {stats['total_errors']} å¤„é”™è¯¯ã€‚")
            st_leaky = st.empty() # ä¸ºæ¼å¡«æ£€æŸ¥åˆ›å»ºä¸€ä¸ªå ä½ç¬¦
            
            # 3. (æ–°) æ˜¾ç¤ºæ‰€æœ‰ä¸‹è½½æŒ‰é’®
            st.divider()
            st.subheader("ğŸ“¤ ä¸‹è½½å®¡æ ¸ç»“æœæ–‡ä»¶")
            
            cols = st.columns(2) # åˆ›å»ºä¸¤åˆ—æ¥æ”¾ç½®ä¸‹è½½æŒ‰é’®
            col_idx = 0
            
            for (filename, data) in all_files:
                if filename and data: 
                    with cols[col_idx % 2]: # è½®æµåœ¨ä¸¤åˆ—ä¸­æ˜¾ç¤º
                        st.download_button(
                            label=f"ğŸ“¥ ä¸‹è½½ {filename}",
                            data=data,
                            file_name=filename,
                            key=f"download_btn_{filename}"
                        )
                    col_idx += 1
            
            # (æ–°) åœ¨ä¸‹è½½æŒ‰é’®ä¸‹æ–¹æ˜¾ç¤ºæ¼å¡«ä¿¡æ¯
            st_leaky.subheader("ğŸ“‹ åˆåŒæ¼å¡«æ£€æµ‹ç»“æœ")
            if stats['leaky_count'] > 0:
                 st_leaky.warning(f"âš ï¸ å…±å‘ç° {stats['leaky_count']} ä¸ªåˆåŒåœ¨æ‰€æœ‰æ£€æŸ¥è¡¨ä¸­æœªå‡ºç°ï¼ˆåŸºäº'ææˆ'sheetï¼‰ã€‚")
            else:
                 st_leaky.success("âœ… æ‰€æœ‰ææˆsheetåˆåŒå·å‡å·²å‡ºç°åœ¨æ£€æŸ¥è¡¨ä¸­ï¼Œæ— æ¼å¡«ã€‚")
            
        except FileNotFoundError as e:
            st.error(f"âŒ æ–‡ä»¶æŸ¥æ‰¾å¤±è´¥: {e}")
            st.info("è¯·ç¡®ä¿æ‚¨ä¸Šä¼ äº†æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶ï¼ˆé¡¹ç›®ææˆã€æ”¾æ¬¾æ˜ç»†ã€äºŒæ¬¡æ˜ç»†ã€äº§å“å°è´¦ï¼‰ã€‚")
            st.session_state.audit_run = False # å‡ºé”™æ—¶é‡ç½®çŠ¶æ€
        except ValueError as e:
            st.error(f"âŒ Sheet æŸ¥æ‰¾å¤±è´¥: {e}")
            st.info("è¯·ç¡®ä¿æ‚¨çš„Excelæ–‡ä»¶åŒ…å«å¿…éœ€çš„ sheet (ä¾‹å¦‚ 'ææˆ')ã€‚")
            st.session_state.audit_run = False
        except Exception as e:
            st.error(f"âŒ å®¡æ ¸è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            st.exception(e)
            st.session_state.audit_run = False
